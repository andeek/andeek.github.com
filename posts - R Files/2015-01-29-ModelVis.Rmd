---
title: "Reading: Visualization of the model fitting process"
layout: "post_ak"
comments: yes
root: ../../../../
---

As part of the course [Stat 503](http://streaming.stat.iastate.edu/~dicook/EDA.and.datamining/) I am taking this semester, I will be posting a series of responses to assigned course readings. Mostly these will be my rambling thoughts as I skim papers.

****

This week we are reading a paper by Hadley Wickham, Dianne Cook, and Heike Hofmann entitled ["Visualizing Statistical Models: Removing the Blindfold"](http://had.co.nz/stat645/model-vis.pdf) which was submitted to the Annals of Applied Statistics. This paper presents multiple ways that visualization can be useful in throughout the model fitting process, rather than solely after the models have been fit and a "best" model has been selected. This paper has been my favorite reading assignment thus far in the semester; I feel as though I'd been thinking of some of these concepts without knowing how to put words to them properly. 

#Strategies
There are three main strategies presented in the paper to enhance model fitting through the use of visualization and graphics:

1. Display the model in the data space
2. Look at all members in the collection
3. Understand the process of model fitting

These three ideas on their own can be very powerful, but together I imagine the improvement to the model fitting would be more than the sum of the parts. To get an idea for all three of these suggestions, I will run through a case study of fitting linear models to the `mtcars` dataset.

#Model in the data space
A common way to diagnose the fit of a model is to display the data in the model space. For example, with linear regression we often look at plots of residuals versus fitted values to assess the assumptions of a linear model. This maps each observation (data) to a point in two dimensions that are summaries from the model (in the model space). The authors suggest flipping this paradigm and mapping the model in the data space instead. One way to accomplish this for a predictive model with continuous response is to visualize the response surface over a grid of predictors.

```{r, results='markup'}
library(ggplot2)
library(dplyr)
library(tidyr)
library(rgl)

m0 <- lm(mpg ~ disp + hp, data = mtcars)
grid <- data.frame(disp = seq(min(mtcars$disp), max(mtcars$disp), len = 50),
           hp = seq(min(mtcars$hp), max(mtcars$hp), len = 50))

grid$pred <- predict(m0, newdata = grid)
grid <- grid[order(grid$disp,grid$hp),]

#plot3d(mtcars[,c("disp", "hp", "mpg")], type="p", col="red", site=5, lwd=15)
#surface3d(grid$disp, grid$hp, matrix(grid$pred, nrow=length(grid$disp), ncol=length(grid$hp)), alpha=.2)
#writeWebGL(dir="images/blog/2015-01-29-ModelVis/")

lines <- readLines("images/blog/2015-01-29-ModelVis/index.html")
#remove canvasMatrix load -- we'll load it elsewhere
lines <- lines[-1]

cat(paste(lines, collapse="\n"))

```


#A look at the members
In many modeling activities, some models are fit within the same family of models (think linear models with a main effect for example) and then a "best" combination of variables is chosen through some criteria (AIC, BIC, etc.). However, the only model that gets explored in depth is this best model. The less optimal models are thrown away in the selection process. The authors argue that by visualizing multiple models in the selection process can give "more insight into the data and the relative merits of different models". To quote John Tukey,

> The  greatest	value	of	a	picture	is	when	it	forces	us	to	notice	what	we	never	expected	to	see.

This is an important concept that can be applied to the model selection process, and is what the authors suggest when they say "look at members in a collection". We can explore multiple models and hopefully find out something we never expected about the data and benefits of different models.

```{r}

```


#Understanding the process
By understanding how a model is fit, and by that I mean the algorithm that fits the model, we can more fully understand how specific data affect the resulting model. This can often be accomplished by visualizing the iterations of a model fitting algorithm to view each step in the process. For example, using the Newton-Raphson method, we can see the steps taken to arrive at a maximum in maximum likelihood fitting for a specific model.
